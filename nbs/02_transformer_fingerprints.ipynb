{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp transformer_fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer fingerprints\n",
    "\n",
    "> RXNBERT fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import pkg_resources\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "from rxnfp.core import (\n",
    "    FingerprintGenerator\n",
    ")\n",
    "from rxnfp.tokenization import (\n",
    "    SmilesTokenizer,\n",
    "    convert_reaction_to_valid_features,\n",
    "    convert_reaction_to_valid_features_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RXNBERTFingerprintGenerator(FingerprintGenerator):\n",
    "    \"\"\"\n",
    "    Generate RXNBERT fingerprints from reaction SMILES\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: BertModel, tokenizer: SmilesTokenizer, force_no_cuda=False):\n",
    "        super(RXNBERTFingerprintGenerator).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if (torch.cuda.is_available() and not force_no_cuda) else \"cpu\")\n",
    "\n",
    "    def convert(self, rxn_smiles: str):\n",
    "        \"\"\"\n",
    "        Convert rxn_smiles to fingerprint\n",
    "\n",
    "        Args:\n",
    "            rxn_smiles (str): precursors>>products\n",
    "        \"\"\"\n",
    "        bert_inputs = convert_reaction_to_valid_features(rxn_smiles, self.tokenizer)\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(\n",
    "                torch.tensor(bert_inputs.input_ids.astype(np.int64)).unsqueeze(0).to(self.device),\n",
    "                torch.tensor(bert_inputs.input_mask.astype(np.int64)).unsqueeze(0).to(self.device),\n",
    "                torch.tensor(bert_inputs.segment_ids.astype(np.int64)).unsqueeze(0).to(self.device),\n",
    "            )\n",
    "\n",
    "        # [CLS] token embeddings\n",
    "        embeddings = output.squeeze()[0].cpu().numpy().tolist()\n",
    "        return embeddings\n",
    "\n",
    "    def convert_batch(self, rxn_smiles_list: List[str]):\n",
    "        bert_inputs = convert_reaction_to_valid_features_batch(\n",
    "            rxn_smiles_list, self.tokenizer\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(\n",
    "                torch.tensor(bert_inputs.input_ids.astype(np.int64)).to(self.device),\n",
    "                torch.tensor(bert_inputs.input_mask.astype(np.int64)).to(self.device),\n",
    "                torch.tensor(bert_inputs.segment_ids.astype(np.int64)).to(self.device),\n",
    "            )\n",
    "\n",
    "        # [CLS] token embeddings in position 0\n",
    "        embeddings = output[:, 0, :].cpu().numpy().tolist()\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class RXNBERTMinhashFingerprintGenerator(FingerprintGenerator):\n",
    "    \"\"\"\n",
    "    Generate RXNBERT fingerprints from reaction SMILES\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model: BertModel, tokenizer: SmilesTokenizer, permutations=256, seed=42, force_no_cuda=False\n",
    "    ):\n",
    "        super(RXNBERTFingerprintGenerator).__init__()\n",
    "        import tmap as tm\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.minhash = tm.Minhash(model.config.hidden_size, seed, permutations)\n",
    "        self.generator = RXNBERTFingerprintGenerator(model, tokenizer)\n",
    "        self.device = torch.device(\"cuda\" if (torch.cuda.is_available() and not force_no_cuda) else \"cpu\")\n",
    "\n",
    "    def convert(self, rxn_smiles: str):\n",
    "        \"\"\"\n",
    "        Convert rxn_smiles to fingerprint\n",
    "\n",
    "        Args:\n",
    "            rxn_smiles (str): precursors>>products\n",
    "        \"\"\"\n",
    "        float_fingerprint = self.generator.convert(rxn_smiles)\n",
    "        minhash_fingerprint = self.minhash.from_weight_array(\n",
    "            float_fingerprint, method=\"I2CWS\"\n",
    "        )\n",
    "        return minhash_fingerprint\n",
    "\n",
    "    def convert_batch(self, rxn_smiles_list: List[str]):\n",
    "        float_fingerprints = self.generator.convert_batch(rxn_smiles_list)\n",
    "        minhash_fingerprints = [\n",
    "            self.minhash.from_weight_array(fp, method=\"I2CWS\")\n",
    "            for fp in float_fingerprints\n",
    "        ]\n",
    "        return minhash_fingerprints\n",
    "    \n",
    "def get_default_model_and_tokenizer(model='bert_ft', force_no_cuda=False):\n",
    "    \n",
    "    model_path =  pkg_resources.resource_filename(\n",
    "                \"rxnfp\",\n",
    "                f\"models/transformers/{model}\"\n",
    "            )\n",
    "\n",
    "    tokenizer_vocab_path = (\n",
    "        pkg_resources.resource_filename(\n",
    "                    \"rxnfp\",\n",
    "                    f\"models/transformers/{model}/vocab.txt\"\n",
    "                )\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if (torch.cuda.is_available() and not force_no_cuda) else \"cpu\")\n",
    "    \n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "    model = model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    tokenizer = SmilesTokenizer(\n",
    "        tokenizer_vocab_path, max_len=model.config.max_position_embeddings\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_fingerprints(rxns: List[str], fingerprint_generator:FingerprintGenerator, batch_size=1) -> np.array:\n",
    "    fps = []\n",
    "\n",
    "    n_batches = len(rxns) // batch_size\n",
    "    emb_iter = iter(rxns)\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch = list(islice(emb_iter, batch_size))\n",
    "\n",
    "        fps_batch = fingerprint_generator.convert_batch(batch)\n",
    "\n",
    "        fps += fps_batch\n",
    "    return np.array(fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convert reaction to fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "[-2.017495632171631, 1.7602037191390991, -1.3323537111282349, -1.109501838684082, 1.2254540920257568]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_default_model_and_tokenizer()\n",
    "\n",
    "rxnfp_generator = RXNBERTFingerprintGenerator(model, tokenizer)\n",
    "\n",
    "example_rxn = \"Nc1cccc2cnccc12.O=C(O)c1cc([N+](=O)[O-])c(Sc2c(Cl)cncc2Cl)s1>>O=C(Nc1cccc2cnccc12)c1cc([N+](=O)[O-])c(Sc2c(Cl)cncc2Cl)s1\"\n",
    "\n",
    "fp = rxnfp_generator.convert(example_rxn)\n",
    "print(len(fp))\n",
    "print(fp[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convert reaction list to fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 256\n"
     ]
    }
   ],
   "source": [
    "fps = rxnfp_generator.convert_batch([example_rxn, example_rxn])\n",
    "print(len(fps), len(fps[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convert reaction to minhash fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "VectorUint[248, 1, 39, 1, 201]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_default_model_and_tokenizer()\n",
    "\n",
    "rxnmhfp_generator = RXNBERTMinhashFingerprintGenerator(model, tokenizer)\n",
    "\n",
    "example_rxn = \"Nc1cccc2cnccc12.O=C(O)c1cc([N+](=O)[O-])c(Sc2c(Cl)cncc2Cl)s1>>O=C(Nc1cccc2cnccc12)c1cc([N+](=O)[O-])c(Sc2c(Cl)cncc2Cl)s1\"\n",
    "\n",
    "fp = rxnmhfp_generator.convert(example_rxn)\n",
    "print(len(fp))\n",
    "print(fp[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
